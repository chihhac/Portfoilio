{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "With no API key: 240 requests per minute, per IP address. 1,000 requests per day, per IP address.\n",
    "\n",
    "With an API key: 240 requests per minute, per key. 120,000 requests per day, per key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your api key [here](https://open.fda.gov/apis/authentication/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maude database\n",
    "\n",
    "### Parameter:\n",
    "    - manufacturer_name (required)\n",
    "        - ex. 'Edwards+Lifesciences'\n",
    "    - zip_code (required)\n",
    "    - product code (optional)\n",
    "    - date_start & date_end (optional)(recommended)\n",
    "        - ex. 20220301\n",
    "    - other_para & other_para_value \n",
    "(For using the other_para, please use the variables in [searchable field](https://open.fda.gov/apis/device/event/searchable-fields/))\n",
    "(Can not get more than 25000 record with the same parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Companies with their postal code\n",
    "- Edwards - 92614\n",
    "- Abbott  - 60064\n",
    "- Boston Scientific Corporation - 01760\n",
    "- Livanova - 77058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pathlib import Path  \n",
    "\n",
    "def maude_extract_by_manu(manufacture_name = None,zip_code=None, product_code = None ,date_start = None, date_end = None, other_para = None, other_para_value = None, limit = 1000):\n",
    "    URL = \"https://api.fda.gov/device/event.json?search=\"\n",
    "    if manufacture_name != None:\n",
    "        URL += f\"device.manufacturer_d_name:'{manufacture_name}'\"\n",
    "        if product_code != None:\n",
    "            URL += f\"+AND+device.device_report_product_code:'{product_code}'\"\n",
    "        if zip_code != None:\n",
    "            URL += f\"+AND+device.manufacturer_d_zip_code:'{zip_code}'\"\n",
    "        if (date_start != None) and (date_end != None):\n",
    "            URL += f\"+AND+date_received:[{date_start}+TO+{date_end}]\"\n",
    "        if (other_para != None) and (other_para_value != None):\n",
    "            URL += f\"+AND+{other_para}:'{other_para_value}'\"\n",
    "            \n",
    "    URL += f\"&limit={limit}\"\n",
    "    json_data = requests.get(URL).json()\n",
    "    data = pd.json_normalize(json_data,'results')\n",
    "    \n",
    "    if json_data['meta']['results']['total'] > 1000:\n",
    "        skip_data = pd.DataFrame()\n",
    "        num = 1000\n",
    "        while num < json_data['meta']['results']['total']:\n",
    "            TEM_URL = URL + f\"&skip={num}\"\n",
    "            skip_json_data = requests.get(TEM_URL).json()\n",
    "            try:\n",
    "                TEM_data = pd.json_normalize(skip_json_data,'results')\n",
    "                skip_data = pd.concat([skip_data,TEM_data])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            num += 1000\n",
    "        return skip_data\n",
    "    if 'skip_data' in globals():\n",
    "        data.append(skip_data)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maude_extract_by_manu_api(api_key ,manufacture_name = None, zip_code = None ,product_code = None ,date_start = None, date_end = None, other_para = None, other_para_value = None, limit = 1000):\n",
    "    URL = \"https://api.fda.gov/device/event.json?\"\n",
    "    URL += f\"api_key={api_key}&search=\"\n",
    "    if manufacture_name != None:\n",
    "        URL += f\"device.manufacturer_d_name.exact:'{manufacture_name}'\"\n",
    "        if zip_code != None:\n",
    "            URL += f\"+AND+device.manufacturer_d_zip_code:'{zip_code}'\"\n",
    "        if product_code != None:\n",
    "            URL += f\"+AND+device.device_report_product_code:'{product_code}'\"\n",
    "        if (date_start != None) and (date_end != None):\n",
    "            URL += f\"+AND+date_received:[{date_start}+TO+{date_end}]\"\n",
    "        if (other_para != None) and (other_para_value != None):\n",
    "            URL += f\"+AND+{other_para}:'{other_para_value}'\"\n",
    "            \n",
    "    URL += f\"&limit={limit}\"\n",
    "    json_data = requests.get(URL).json()\n",
    "    data = pd.json_normalize(json_data,'results')\n",
    "    \n",
    "\n",
    "    if json_data['meta']['results']['total'] > 1000:\n",
    "        skip_data = pd.DataFrame()\n",
    "        num = 1000\n",
    "        while num < json_data['meta']['results']['total']:\n",
    "            TEM_URL = URL + f\"&skip={num}\"\n",
    "            skip_json_data = requests.get(TEM_URL).json()\n",
    "            try:\n",
    "                TEM_data = pd.json_normalize(skip_json_data,'results')\n",
    "                skip_data = pd.concat([skip_data,TEM_data])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            num += 1000\n",
    "        return skip_data\n",
    "    if 'skip_data' in globals():\n",
    "        data.append(skip_data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maude_extract_by_product(product_code = None ,date_start = None, date_end = None, other_para = None, other_para_value = None, limit = 1000):\n",
    "    URL = \"https://api.fda.gov/device/event.json?search=\"\n",
    "    if product_code != None:\n",
    "        URL += f\"device.device_report_product_code:'{product_code}'\"\n",
    "        if (date_start != None) and (date_end != None):\n",
    "            URL += f\"+AND+date_received:[{date_start}+TO+{date_end}]\"\n",
    "        if (other_para != None) and (other_para_value != None):\n",
    "            URL += f\"+AND+{other_para}:'{other_para_value}'\"\n",
    "            \n",
    "    URL += f\"&limit={limit}\"\n",
    "    json_data = requests.get(URL).json()\n",
    "    data = pd.json_normalize(json_data,'results')\n",
    "    \n",
    "\n",
    "    if json_data['meta']['results']['total'] > 1000:\n",
    "        skip_data = pd.DataFrame()\n",
    "        num = 1000\n",
    "        while num < json_data['meta']['results']['total']:\n",
    "            TEM_URL = URL + f\"&skip={num}\"\n",
    "            skip_json_data = requests.get(TEM_URL).json()\n",
    "            try:\n",
    "                TEM_data = pd.json_normalize(skip_json_data,'results')\n",
    "                skip_data = pd.concat([skip_data,TEM_data])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            num += 1000\n",
    "        return skip_data\n",
    "    if 'skip_data' in globals():\n",
    "        data.append(skip_data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def ymd_to_y_m_d(d):\n",
    "    return datetime.strptime(d, '%Y%m%d').strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_ymd_to_y_m_d(dataframe, col):\n",
    "\n",
    "    for i in range(len(dataframe)):\n",
    "        if isinstance(dataframe.loc[i,col],str) != True:\n",
    "            dataframe.loc[i,col] = None\n",
    "    for i in range(len(dataframe)):\n",
    "        if (dataframe.loc[i,col] != None) and (dataframe.loc[i,col] != \"\"):\n",
    "            dataframe.loc[i,col] = ymd_to_y_m_d(dataframe.loc[i,col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maude_data(dataframe):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    data = dataframe\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df['report_number'] = data['report_number']\n",
    "    df['date_of_event'] = data['date_of_event']\n",
    "    df['event_type'] = data['event_type']\n",
    "    df['manufacturer_name'] = data['manufacturer_g1_name']\n",
    "    df['date_received'] = data['date_received']\n",
    "    df['product_problem'] = data['product_problems']\n",
    "    df['zip_code'] = data['manufacturer_contact_zip_code']\n",
    "    for i in range(len(data)):\n",
    "        if data['device'][i][0]['device_report_product_code'] != None:\n",
    "            df.loc[i,'prodcut_code'] = data['device'][i][0]['device_report_product_code']\n",
    "        if data['device'][i][0]['brand_name'] != None:\n",
    "            df.loc[i,'brand_name'] = data['device'][i][0]['brand_name']\n",
    "        if data['device'][i][0]['model_number'] != None:\n",
    "            df.loc[i,'model_number'] = data['device'][i][0]['model_number']\n",
    "        if(len(data['patient'][i])>0):\n",
    "            if \"patient_problems\" in data['patient'][i][0]:\n",
    "            \n",
    "                if data['patient'][i][0]['patient_problems'] != None:\n",
    "                #print(\", \".join(data['patient'][i][0]['patient_problems']))\n",
    "                \n",
    "                    df.loc[i,'patient_problem'] = \", \".join(data['patient'][i][0]['patient_problems'])\n",
    "        if len(data['mdr_text'][i]) >= 2:\n",
    "            for j in range(2):\n",
    "                if data['mdr_text'][i][j]['text_type_code'] != None:\n",
    "                    if data['mdr_text'][i][j]['text_type_code'] == 'Additional Manufacturer Narrative':\n",
    "                        df.loc[i,'Manufacture_Narrative'] = data['mdr_text'][i][j]['text']\n",
    "                    if data['mdr_text'][i][j]['text_type_code'] == 'Description of Event or Problem':\n",
    "                        df.loc[i,'Event_Description'] = data['mdr_text'][i][j]['text']\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        for col in df.columns:\n",
    "            if df.loc[i,col] == \"\":\n",
    "                df.loc[i,col] = None\n",
    "    df.dropna(subset=['model_number'],inplace=True)\n",
    "    \n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df['model_number'] = df['model_number'].astype(str).str[0:4]\n",
    "    series_ymd_to_y_m_d(dataframe=df, col= 'date_of_event')\n",
    "    series_ymd_to_y_m_d(dataframe=df, col= 'date_received')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract MAUDE data by company example - Edwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we will use company's name, its postal code, the time you want to extract to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edwards = maude_extract_by_manu(manufacture_name='Edwards+Lifesciences',zip_code=92614,date_end=20220331, date_start=20180101)\n",
    "edwards = maude_data(edwards)\n",
    "edwards.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data as cvs file\n",
    "from pathlib import Path\n",
    "\n",
    "#put the file path where you want to save in the filepath variable\n",
    "# ex. \"/Users/sam/Desktop/UCI/Capstone/Maude/data/Edwards/Edwards_2018to2022.csv\"\n",
    "filepath = Path('')\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "edwards.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract MAUDE data by company example - Boston Scientific Corporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the company may have multiple postal code due to different factory or office.\n",
    "\n",
    "In this time, we will extract the data by their name and do some cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = maude_extract_by_manu(manufacture_name='boston+scientific', date_start=20180101, date_end=20220331)\n",
    "boston = maude_data(boston)\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.dropna(subset = ['manufacturer_name'],inplace = True)\n",
    "boston = boston[boston.manufacturer_name.str.contains('BOSTON SCIENTIFIC')]\n",
    "boston.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data as cvs file\n",
    "from pathlib import Path\n",
    "\n",
    "#put the file path where you want to save in the filepath variable\n",
    "# ex. \"/Users/sam/Desktop/UCI/Capstone/Maude/data/Edwards/Edwards_2018to2022.csv\"\n",
    "filepath = Path('')\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "boston.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract MAUDE data by company - Livanova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livanova = maude_extract_by_manu(manufacture_name='livanova', date_start= 20180101, date_end=20220331)\n",
    "livanova = maude_data(livanova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('')\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "livanova.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract MAUDE data by company - Medtronic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medtronic = maude_extract_by_manu(manufacture_name='medtronic', date_start= 20180101, date_end=20220331)\n",
    "medtronic = maude_data(medtronic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path('')\n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "medtronic.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract MAUDE data by product code example - NPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NPT = maude_extract_by_product(product_code='NPT',date_start=20180101, date_end=20220331)\n",
    "NPT = maude_data(NPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract MAUDE data by product code example - DYE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For product code extraction function, we use only the product code and the date to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DYE = maude_extract_by_product(product_code='DYE',date_start=20180101, date_end=20220331)\n",
    "DYE = maude_data(DYE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract MAUDE data by product code - LWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LWR = maude_extract_by_product(product_code='LWR', date_start=20180101, date_end=20220331)\n",
    "LWR = maude_data(LWR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall database\n",
    "\n",
    "### Parameter:\n",
    "    - recall firm (required)\n",
    "    - product code (optional)\n",
    "    - date_start & date_end (optional)\n",
    "    - other_para & other_para_value \n",
    "(For using the other_para, please use the variables in [searchable field](https://open.fda.gov/apis/device/recall/searchable-fields/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def recall_extract_by_firm(recall_firm = None,date_start = None, date_end = None, other_para = None, other_para_value = None, limit = 1000):\n",
    "    URL = \"https://api.fda.gov/device/recall.json?search=\"\n",
    "    if recall_firm != None:\n",
    "        URL += f\"recalling_firm:'{recall_firm}'\"\n",
    "        if (date_start != None) and (date_end != None):\n",
    "            URL += f\"+AND+event_date_initiated:[{date_start}+TO+{date_end}]\"\n",
    "        if (other_para != None) and (other_para_value != None):\n",
    "            URL += f\"+AND+{other_para}:'{other_para_value}'\"\n",
    "            \n",
    "    URL += f\"&limit={limit}\"\n",
    "    json_data = requests.get(URL).json()\n",
    "    data = pd.json_normalize(json_data,'results')\n",
    "    \n",
    "    if json_data['meta']['results']['total'] > 1000:\n",
    "        skip_data = pd.DataFrame()\n",
    "        num = 1000\n",
    "        while num < json_data['meta']['results']['total']:\n",
    "            TEM_URL = URL + f\"&skip={num}\"\n",
    "            skip_json_data = requests.get(TEM_URL).json()\n",
    "            try:\n",
    "                TEM_data = pd.json_normalize(skip_json_data,'results')\n",
    "                skip_data = pd.concat([skip_data,TEM_data])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            num += 1000\n",
    "        return skip_data\n",
    "    if 'skip_data' in globals():\n",
    "        data.append(skip_data)\n",
    "    \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def trans_recall(data1, recall_firm):\n",
    "    data1.dropna(subset = ['recalling_firm'], inplace = True)\n",
    "    data = data1[data1['recalling_firm'].str.lower().str.contains(f\"{recall_firm.replace('+',' ').lower()}\")]\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['recall_number'] = data['product_res_number']\n",
    "    df['product_code'] = data['product_code']\n",
    "    df['product_description'] = data['product_description']\n",
    "    df['firm_name'] = data['recalling_firm']\n",
    "    df['termination_date'] = data['event_date_terminated']\n",
    "    df['posted_internet_date'] = data['event_date_posted']\n",
    "    df['center_classification_date'] = data['event_date_initiated']\n",
    "    df['root_cause_description'] = data['root_cause_description']\n",
    "    df['recall_status'] = data['recall_status']\n",
    "    df['action'] = data['action']\n",
    "    df['recall_class'] = data['openfda.device_class']\n",
    "    df['recall_reason'] = data['reason_for_recall']\n",
    "    df['recall_class'] = df['recall_class'].replace(['1','3'],['3','1'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#replace the recall class from [1,3] to [3,1], because the recall class we extracted using API is different from what FDA usually used\n",
    "#For class 1 in API, it's class III for what FDA usually uses\n",
    "def recall_class_trans(dataframe):\n",
    "    dataframe['recall_class'] = dataframe['recall_class'].replace([1, 3],[3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Recall data by company example - Edwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_edwards = recall_extract_by_firm(recall_firm='edwards+lifesciences')\n",
    "recall_edwards = trans_recall(recall_firm='edwards+lifesciences', data1 = recall_edwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edwards_productcode = ['NPT', 'DYE', 'LWR', 'NPU', 'DXO', 'DYG', 'DXE', 'DXG' , 'LDF', 'DQE']\n",
    "recall_edwards = recall_edwards.loc[recall_edwards.product_code.isin(edwards_productcode),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Recall data by company example - Boston Scientific Corporation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall_boston = recall_extract_by_firm(recall_firm= 'boston+scientific+corporation')\n",
    "recall_boston = trans_recall(data1=recall_boston, recall_firm='boston+scientific+corporation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_productcode = ['NIQ', 'LWP', 'LWS', 'NVN', 'LOX', 'LIT', 'NGV', 'DQY', 'NWX', 'MCX', 'DTB', 'NVY', 'NIK', 'DQX', 'OBJ', 'NPT']\n",
    "recall_boston = recall_boston.loc[recall_boston.product_code.isin(boston_productcode),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Recall data by company example - Abbott"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 2017, St. Jude had been merged by Abbott. Therefore, we also extract St. Jude data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall_abbott = recall_extract_by_firm(recall_firm='abbott+laboratories')\n",
    "recall_abbott = trans_recall(recall_firm='abbott', data1= recall_abbott)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbott_productcode = ['DSQ', 'NIQ','LWS','LOX','NIK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_abbott = recall_abbott.loc[recall_abbott.product_code.isin(abbott_productcode),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_jude = recall_extract_by_firm(recall_firm='jude')\n",
    "recall_jude = trans_recall(data1=recall_jude, recall_firm='jude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jude_productcode = ['NVN', 'NIK', 'NVY', 'NVZ', 'LWP', 'LWS','MXC', 'MOM','DYB','LWR','OJX','OAE','DRF',\n",
    "'DXY','NKE','DQK','LWQ','OAD','DRC','DTB','DQX','OBJ','DQO','LPB','DRA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_jude = recall_jude.loc[recall_jude.product_code.isin(jude_productcode),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_abbott_final = pd.concat([recall_abbott, recall_jude])\n",
    "recall_abbott_final.drop_duplicates(keep='last',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Recall data by company - Livanova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall_livanova = recall_extract_by_firm(recall_firm = 'livanova')\n",
    "recall_livanova = trans_recall(data1= recall_livanova, recall_firm= 'livanova')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livanova_productcode = ['DWE', 'DWF', 'DWC', 'DTS', 'DTL', 'DTZ', 'LWR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_livanova = recall_livanova.loc[recall_livanova.product_code.isin(livanova_productcode),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Recall data by company - Medtronic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall_medtronic = recall_extract_by_firm(recall_firm='medtronic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medtronic_productcode = ['NVZ', 'LWS', 'NPT', 'NIK', 'DSI', 'DTB', 'MIH', 'KRG', 'NIQ', 'PNJ', 'DYE', 'LWR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_medtronic = recall_medtronic.loc[recall_medtronic.product_code.isin(medtronic_productcode),:]\n",
    "recall_medtronic = trans_recall(data1= recall_medtronic, recall_firm='medtronic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "def extract_warning_letter_list(search):\n",
    "    url = f\"https://www.fda.gov/datatables/views/ajax?search_api_fulltext={search}&search_api_fulltext_issuing_office=&field_letter_issue_datetime=All&field_change_date_closeout_letter=&field_change_date_response_letter=&field_change_date_2=All&field_letter_issue_datetime_2=&draw=4&columns%5B0%5D%5Bdata%5D=0&columns%5B0%5D%5Bname%5D=&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=1&columns%5B1%5D%5Bname%5D=&columns%5B1%5D%5Bsearchable%5D=true&columns%5B1%5D%5Borderable%5D=true&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=2&columns%5B2%5D%5Bname%5D=&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=true&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=3&columns%5B3%5D%5Bname%5D=&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=true&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=4&columns%5B4%5D%5Bname%5D=&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=true&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=5&columns%5B5%5D%5Bname%5D=&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=true&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=6&columns%5B6%5D%5Bname%5D=&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=true&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=7&columns%5B7%5D%5Bname%5D=&columns%5B7%5D%5Bsearchable%5D=true&columns%5B7%5D%5Borderable%5D=false&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&start=0&length=10&search%5Bvalue%5D=&search%5Bregex%5D=false&_drupal_ajax=1&_wrapper_format=drupal_ajax&pager_element=0&view_args=&view_base_path=inspections-compliance-enforcement-and-criminal-investigations%2Fcompliance-actions-and-activities%2Fwarning-letters%2Fdatatables-data&view_display_id=warning_letter_solr_block&view_dom_id=216facc099c04af0f56180f89afb9f1d0a008020ee32fb027a8c49bc45f91f90&view_name=warning_letter_solr_index&view_path=%2Finspections-compliance-enforcement-and-criminal-investigations%2Fcompliance-actions-and-activities%2Fwarning-letters&total_items=3033&_=1653280484120\"\n",
    "    text = requests.get(url)\n",
    "    search_response = json.loads(text.text)[\"data\"]\n",
    "    print(search_response)\n",
    "    list_of_urls = list()\n",
    "    for i, text in enumerate(search_response):\n",
    "        if \"<a href\" in text[2]:\n",
    "            list_of_urls.append(text[2].split(\"<a href=\")[1].split(\">\")[0][1:-1])\n",
    "    \n",
    "    url_base = \"https://www.fda.gov\"\n",
    "    urls_to_be_fired = list()\n",
    "    for i, url in enumerate(list_of_urls):\n",
    "        url_final = url_base + url\n",
    "        urls_to_be_fired.append(url_final)\n",
    "    \n",
    "    return urls_to_be_fired\n",
    "\n",
    "def web_scrape_warning_letter(list_name):\n",
    "\n",
    "    web_text = list()\n",
    "    for i, url in enumerate(list_name):\n",
    "        page = urlopen(url)\n",
    "        html_bytes = page.read()\n",
    "        html = html_bytes.decode(\"utf-8\")\n",
    "        start_index = html.find(\">WARNING LETTER</\")\n",
    "        end_index = html.find(\"/S/\")\n",
    "        title = html[start_index:end_index]\n",
    "        title = title.split('</div><div>\\xa0</div><div>')\n",
    "        cleantext = list()\n",
    "        for i, text in enumerate(title):\n",
    "            TEMP = BeautifulSoup(text, \"html.parser\").text\n",
    "            TEMP = re.sub(\"\\xa0\", \" \", TEMP)\n",
    "            TEMP = re.sub(\"\\n\", '', TEMP)\n",
    "            cleantext.append(TEMP)\n",
    "        #cleantext = cleantext[cleantext.find('Dear'):cleantext.find('Sincerely')]\n",
    "        web_text.append(cleantext)\n",
    "    return web_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jude_list = extract_warning_letter_list(search='jude')\n",
    "jude = web_scrape_warning_letter(jude_list)\n",
    "jude1 = jude[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medtronic_list = extract_warning_letter_list(search='medtronic')\n",
    "medtronic_warning = web_scrape_warning_letter(medtronic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.update(['caon', \"firm’s\", \"firm\", \"analysis\",\"failed\",'dear','mr','rousseau',\"rousseau:\" \"during\",'will'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(jude1):\n",
    "    jude1[i] = text.lower()\n",
    "\n",
    "for i, text in enumerate(jude1):\n",
    "    jude1[i] = re.sub('[,\\.!?â€™]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(jude1):\n",
    "    lemmatized_sentence = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    jude1[i] = lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using N-gram to know more about warning letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while using n-gram, we are looking for device category or name. For instance, battery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n_gram = 1):\n",
    "    token = [token for token in text.lower().split(' ') if token != '' if token not in stopwords]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [' '.join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join([token for token in jude1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "unigram = defaultdict(int)\n",
    "for word in generate_ngrams(text):\n",
    "    unigram[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_unigram = pd.DataFrame(sorted(unigram.items(), key= lambda x : x[1])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(y=df_unigram[0].values[:15], x = df_unigram[1].values[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### bigram\n",
    "\n",
    "bigram = defaultdict(int)\n",
    "for word in generate_ngrams(text, n_gram= 2):\n",
    "    bigram[word] += 1\n",
    "\n",
    "df_bigram = pd.DataFrame(sorted(bigram.items(), key= lambda x:x[1])[::-1])\n",
    "\n",
    "sns.barplot( y = df_bigram[0].values[:15], x = df_bigram[1].values[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trigram\n",
    "\n",
    "trigram = defaultdict(int)\n",
    "for word in generate_ngrams(text, n_gram = 3):\n",
    "    trigram[word] += 1\n",
    "\n",
    "df_trigram = pd.DataFrame(sorted(trigram.items(), key= lambda x: x[1])[::-1])\n",
    "\n",
    "sns.barplot( y = df_trigram[0].values[:15], x = df_trigram[1].values[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using specific word we found in n-gram to know more about the warning letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(a_str, sub):\n",
    "    start = 0\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return\n",
    "        yield start\n",
    "        start += len(sub)\n",
    "\n",
    "def find_specific(word, text_list):\n",
    "    specific = list()\n",
    "    for i, text in enumerate(text_list):\n",
    "        if word.lower() in text:\n",
    "            specific_index = list(find_all(text, word.lower()))\n",
    "            for a in specific_index:\n",
    "                print(i, a)\n",
    "                print(text_list[i][(a-15):])\n",
    "            specific.append(text_list[i][(a-15):])\n",
    "    \n",
    "    return specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use battery as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battery = find_specific(word='battery', text_list=jude1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trigram\n",
    "\n",
    "trigram = defaultdict(int)\n",
    "for word in generate_ngrams(battery[0], n_gram = 3):\n",
    "    trigram[word] += 1\n",
    "\n",
    "df_trigram = pd.DataFrame(sorted(trigram.items(), key= lambda x: x[1])[::-1])\n",
    "\n",
    "sns.barplot( y = df_trigram[0].values[:15], x = df_trigram[1].values[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic modeling for warning letter (not useful cuz the number of data is too small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tools\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# TfidfVectorizer\n",
    "\n",
    "def getTopics(text, topicsNum):\n",
    "    topic = ''\n",
    "    vectorizer = TfidfVectorizer(stop_words= 'english',\n",
    "                                max_features = 1000,\n",
    "                                max_df = 0.8,\n",
    "                                min_df = 0.05)\n",
    "    #temp_list = list()\n",
    "    #temp_list.append(text)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "\n",
    "    print(X.shape)\n",
    "    svd_model = TruncatedSVD(n_components=topicsNum, algorithm='randomized', n_iter= 100, random_state= 122)\n",
    "    svd_model.fit(X)\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    print('components output shape '+ str(svd_model.components_.shape))\n",
    "    print(svd_model.components_)\n",
    "\n",
    "    for i, comp in enumerate(svd_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse= True)[:10]\n",
    "        string = \"Topic \"+str(i+1) +\": \"\n",
    "        for t in sorted_terms:\n",
    "            string = string + t[0] + ' '\n",
    "            topic = topic + t[0] + ' '\n",
    "        print(string)\n",
    "\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the text with battery to do topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTopics(battery,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from heapq import nlargest\n",
    "punctuations = string.punctuation\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer') # updated\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the bullet point of the warning letter using the get_point_warning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_warning(text_list):\n",
    "    a = 0\n",
    "    for i, text in enumerate(text_list):\n",
    "        if text[0].isnumeric() and (int(text[0]) > a):\n",
    "            print(text[:(text.find(',',2))])\n",
    "            a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_point_warning(jude1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_point_warning(medtronic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary the whole warning letter(need some data to tune the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(document):\n",
    "    clean_tokens = [ token.lemma_.lower().strip() for token in document ]\n",
    "    clean_tokens = [ token for token in clean_tokens if token not in STOP_WORDS and token not in punctuations ]\n",
    "    tokens = [token.text for token in document]\n",
    "    lower_case_tokens = list(map(str.lower, tokens))\n",
    "    \n",
    "    return lower_case_tokens\n",
    "\n",
    "\n",
    "def generate_numbers_vector(tokens):\n",
    "    frequency = [tokens.count(token) for token in tokens]\n",
    "    token_dict = dict(list(zip(tokens,frequency)))\n",
    "    maximum_frequency=sorted(token_dict.values())[-1]\n",
    "    normalised_dict = {token_key:token_dict[token_key]/maximum_frequency for token_key in token_dict.keys()}\n",
    "    return normalised_dict\n",
    "\n",
    "def sentences_importance(text, normalised_dict):\n",
    "    importance ={}\n",
    "    for sentence in nlp(text).sents:\n",
    "        for token in sentence:\n",
    "            target_token = token.text.lower()\n",
    "            if target_token in normalised_dict.keys():\n",
    "                if sentence in importance.keys():\n",
    "                    importance[sentence]+=normalised_dict[target_token]\n",
    "                else:\n",
    "                    importance[sentence]=normalised_dict[target_token]\n",
    "    return importance\n",
    "\n",
    "def generate_summary(rank, text):\n",
    "    target_document = parser(text)\n",
    "    importance = sentences_importance(text, generate_numbers_vector(pre_process(target_document)))\n",
    "    summary = nlargest(rank, importance, key=importance.get)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 3\n",
    "for i, text in enumerate(corrective_action):\n",
    "    print(generate_summary(num, text))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
